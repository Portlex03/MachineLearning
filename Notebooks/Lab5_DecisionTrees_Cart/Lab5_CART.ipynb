{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Б). Реализовать обучение и вычисление дерева с использованием алгоритма CART для задачи регрессии и задачи классификации. Выполнить оценку качества моделей, визуализировать дерево решений, вывести решающие правила."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм CART\n",
    "Строит бинарное дерево, где в узлах находится предикат, в листах находится ответ.\n",
    "Задача -- минимизировать ошибку на каждом листе.\n",
    "\n",
    "Алгоритм обучения упрощённо можно описать следующим образом:\n",
    "\n",
    "0. Проверяем критерий остановки\n",
    "1. Строим всевозможные разбиения на две подвыборки по одному признаку\n",
    "2. Выбираем лучшее разбиение\n",
    "3. Возвращаемся к шагу 0 для потомков\n",
    "4. Проводим отсечение (pruning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "IG = S_0 - \\sum_{i=1}^q \\frac{N_i}{N}S_i - Энтропия\n",
    "\\end{align*}\n",
    "\n",
    "$q$ - количество подвыборок (в бинарном дереве две)\n",
    "\n",
    "$IG$ - информационный выигрыш\n",
    "\n",
    "$S_0$ - начальная энтропия\n",
    "\n",
    "$N_i$ - элементы до порога\n",
    "\n",
    "$N$ - все элементы выборки\n",
    "\n",
    "$S_i$ - энтропия элементов\n",
    "\n",
    "\\begin{align*}\n",
    "S = - \\sum_{i=1}^N p_i \\log_2{p_i}\n",
    "\\end{align*}\n",
    "\n",
    "$N$ - количество классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CART:\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_depth: int | None = None, \n",
    "                 min_samples_split: int = 2,\n",
    "                 classification: bool = False,\n",
    "                 ) -> None:\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.classification = classification\n",
    "\n",
    "    class Node:\n",
    "        def __init__(self, \n",
    "                     feature: int | None = None, \n",
    "                     threshold: float | None = None, \n",
    "                     value: int | float | None = None, \n",
    "                     left = None, right = None\n",
    "                     ) -> None:\n",
    "                self.feature = feature\n",
    "                self.threshold = threshold\n",
    "                self.value = value\n",
    "                self.left = left\n",
    "                self.right = right\n",
    "\n",
    "    def _entropy(self, Y: np.ndarray) -> float:\n",
    "        \"\"\"Находит энтропию столбца\"\"\"\n",
    "        probabilities = np.array(list(Counter(Y).values())) / len(Y)\n",
    "        return -np.sum(probabilities * np.log2(probabilities))\n",
    "    \n",
    "    def _MSE(self, Y: np.ndarray) -> float:\n",
    "        \"\"\"Находит среднеквадратичную ошибку столбца\"\"\"\n",
    "        return np.mean((Y - np.mean(Y))**2)\n",
    "\n",
    "    def _split_dataset(self, X: np.ndarray, Y: np.ndarray, feature: int, threshold: float):\n",
    "        \"\"\"\n",
    "        Разделяет датасеты на левую и правую подвыборку \n",
    "        по признаку feature на основе порога threshold\n",
    "        \"\"\"\n",
    "        left_indexes = np.where(X[:,feature] <= threshold)[0]\n",
    "        right_indexes = np.where(X[:,feature] > threshold)[0]\n",
    "        return X[left_indexes], Y[left_indexes], X[right_indexes], Y[right_indexes]\n",
    "\n",
    "    def _find_best_split(self, X: np.ndarray, Y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Находит лучшее разделение данных на левую и правую подвыборку\n",
    "        \"\"\"\n",
    "        best_feature, best_threshold, best_score = None, None, np.inf\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:,feature])\n",
    "            for threshold in thresholds:\n",
    "                x_left, y_left, x_right, y_right = self._split_dataset(X,Y,feature,threshold)\n",
    "\n",
    "                if self.classification:\n",
    "                    score = (len(y_left) * self._entropy(y_left) + \\\n",
    "                             len(y_right) * self._entropy(y_right)) / len(Y)\n",
    "                else:\n",
    "                    score = (len(y_left) * self._MSE(y_left) + \\\n",
    "                             len(y_right) * self._MSE(y_right)) / len(Y)\n",
    "                if score < best_score:\n",
    "                    best_feature, best_threshold, best_score = feature, threshold, score\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _build_tree(self, X: np.ndarray, Y: np.ndarray, depth=0) -> Node:\n",
    "        if depth == self.max_depth or len(X) < self.min_samples_split:\n",
    "            if self.classification:\n",
    "                return self.Node(value=Counter(Y).most_common(1)[0][0])\n",
    "            else:\n",
    "                return self.Node(value=np.mean(Y))\n",
    "            \n",
    "        feature, threshold = self._find_best_split(X,Y)\n",
    "        x_left, y_left, x_right, y_right = self._split_dataset(X,Y,feature,threshold)\n",
    "        left_child = self._build_tree(x_left,y_left,depth=depth + 1)\n",
    "        right_child = self._build_tree(x_right,y_right,depth=depth + 1)\n",
    "\n",
    "        return self.Node(feature=feature,threshold=threshold,left=left_child,right=right_child)\n",
    "\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray):\n",
    "        self.tree_ = self._build_tree(X,Y)\n",
    "        return self\n",
    "\n",
    "    def _predict_single(self, x: np.ndarray, node: Node) -> int | float:\n",
    "        if node.feature is None:\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._predict_single(x, node.left)\n",
    "        else:\n",
    "            return self._predict_single(x, node.right)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> list[float] | list[int]:\n",
    "        y_pred = np.zeros(len(X), dtype=int if self.classification else float)\n",
    "        for i in range(X.shape[0]):\n",
    "            y_pred[i] = self._predict_single(X[i], self.tree_)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка алгоритма CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x,y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_clusters_per_class=1,\n",
    "    n_features=4,\n",
    "    n_classes=3\n",
    ")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.90       128\n",
      "           1       0.90      0.91      0.91       141\n",
      "           2       0.84      0.82      0.83       131\n",
      "\n",
      "    accuracy                           0.88       400\n",
      "   macro avg       0.88      0.88      0.88       400\n",
      "weighted avg       0.88      0.88      0.88       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "crt = CART(classification=True)\n",
    "crt.fit(x_train,y_train)\n",
    "predict = crt.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88       128\n",
      "           1       0.90      0.87      0.89       141\n",
      "           2       0.81      0.83      0.82       131\n",
      "\n",
      "    accuracy                           0.86       400\n",
      "   macro avg       0.86      0.86      0.86       400\n",
      "weighted avg       0.86      0.86      0.86       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_train,y_train)\n",
    "\n",
    "pred = dt.predict(x_test)\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "x,y = make_regression(\n",
    "    n_samples=2000,\n",
    "    n_features=4\n",
    ")\n",
    "\n",
    "x_train, y_train, x_test, y_test = train_test_split(x,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Python\\Lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MSE = 0.32\n",
      "MAE = 0.2\n",
      "RMSE = 0.565685424949238\n",
      "MAPE = 258956978573803.56\n",
      "R^2 = 0.5057485688911199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from my_methods import regression_metrics\n",
    "crt = CART()\n",
    "crt.fit(x_train,y_train)\n",
    "predict = crt.predict(x_test)\n",
    "regression_metrics('',y_test,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MSE = 0.27\n",
      "MAE = 0.175\n",
      "RMSE = 0.5196152422706632\n",
      "MAPE = 213920982300098.62\n",
      "R^2 = 0.5829753550018824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor()\n",
    "dt.fit(x_train,y_train)\n",
    "\n",
    "predict = dt.predict(x_test)\n",
    "regression_metrics('',y_test,predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
